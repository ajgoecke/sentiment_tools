---
title: "Comparison of Sentiment Tools across Domains"
output:
  html_document:
    df_print: paged
---
```{r}
# load required libraries

# to use harry potter dataset
# devtools::install_github("bradleyboehmke/harrypotter")
# devtools::install_github("quanteda/quanteda.sentiment")
# devtools::install_github("quanteda/quanteda.corpora")

library(quanteda)
library(readtext)
library(corpus)
library(tidyverse)
library(stringr)
library(tidytext)
library(harrypotter)
library(dplyr)
library(quanteda.sentiment)
library(vader)
library(caret)
library(reshape2)


require(quanteda)
require(quanteda.corpora)
require(quanteda.sentiment)
#library("quanteda", warn.conflicts = FALSE, quietly = TRUE)
```

# 1. Step: Load Data & Lexicons
```{r}
# load datasets
reviews <- readRDS(file="datasets/red_review.rds")
twitter <- readRDS(file="datasets/red_twitter.rds")
parlvote <- readRDS(file="datasets/red_parl_vote.rds")
amazon <- readRDS(file="datasets/red_ama.rds")

# load lexicons
afinn <- data_dictionary_AFINN
lsd <- data_dictionary_LSD2015
```

```{r}
reviews
twitter
parlvote
amazon
```
# 2. Step: Perform Sentiment Analysis
### Normalize Scores
```{r}
# DATA NORMALIZATION
# Normalize data via minimun/maximum normalization, either by scaling values from 0 to 1 or -1 to 1
# 
# Arg:
#   x: input values (e.g. column of data frame)
#   
# Returns: 
#   normalized data


# min/max normalization from 0 to 1
normalize <- function(x, na.rm = TRUE){
  return((x-min(x)) / (max(x)-min(x)))}

# min/max normalization from -1 to 1
normalize2 <- function(x, na.rm = TRUE){
  return(2* ((x - min(x)) / (max(x)-min(x)))-1)}
```

### Calculate Sentiment Scores
```{r}
# Calculate sentiment scores for different lexicons and input data frames
# 
# Arg:
#   data: input data frame
#   lexicons: names of lexicons that should be used for sentiment scoring
#   normalize: TRUE or FALSE, to optionally normalize sentiment scores
#   
# Returns: 
#   Data frame with (normalized) sentiment sores for chosen lexicons

get_sentiment <- function(df, lexicons, normalize = TRUE){
  
  # for each lexicon, get sentiment scores and save scores in new column of data frame
  for(lex in lexicons){
    
    if(lex == "afinn"){
    df$afinn <- round(textstat_valence(tokens(df$text), afinn, normalize="dictionary")$sentiment,3)}
    
    if(lex == "lsd"){
    df$lsd<- round(textstat_polarity(tokens(df$text), lsd)$sentiment,3)}
    
    if(lex == "vader"){
    df$vader <- round(vader_df(df$text)$compound,3)}
  }
  
  # normalize sentiment scores if TRUE, VADER scores are already normalized
  if(normalize==TRUE){
    df$afinn <- round(normalize2(df$afinn), 3)
    df$lsd <- round(normalize2(df$lsd), 3)
  }
  
  # set sentiment scores to 0 if NA
  df[is.na(df)] <- 0
  
  return(df)
}

reviews_sentiment <- get_sentiment(reviews, c("afinn","lsd", "vader"))
twitter_sentiment <- get_sentiment(twitter, c("afinn","lsd", "vader"))
parlvote_sentiment <- get_sentiment(parlvote, c("afinn","lsd", "vader"))
amazon_sentiment <- get_sentiment(amazon, c("afinn", "lsd", "vader"))

# to save data frame as csv file
#write.csv(parlvote_sentiment, "datasets/parlvote_sentiment.csv", row.names = FALSE)

reviews_sentiment
```
```{r}
# write data to csv file 
#write.csv(parlvote_sentiment, "datasets/sentiment_data/parlvote_sentiment.csv", row.names = FALSE)
#write.csv(amazon_sentiment, "datasets/sentiment_data/amazon_sentiment.csv", row.names = FALSE)
#write.csv(twitter_sentiment, "datasets/sentiment_data/twitter_sentiment.csv", row.names = FALSE)
#write.csv(reviews_sentiment, "datasets/sentiment_data/reviews_sentiment_norm.csv", row.names = FALSE)
```

# 3. Step: Calculate Statistics
### Convert data into ternary format
```{r}
# Convert final values into ternary (1 = positive, 0 = neutral, -1 = negative) format for evaluation and comparison
# 
# Arg:
#   df: input data frame that contains the columns to be converted into ternary format
#   to_change: column names that should be converted into ternary format
#   gold: goldstandard, i.e. rating, which has to be changed to ternary format
#   
# Returns: 
#   data frame with converted values

get_discrete <- function(df, to_change, gold){
  df %>% 
    mutate_at(to_change, function(x){
      # mutate values greater than 0 to 1 (positive), equal to 0 to 0 (neutral) and smaller than 0 to -1 (negative)
      case_when(x > 0 ~ 1, x < 0 ~ -1, x == 0 ~ 0)}) %>% 
    
    mutate_at(gold, function(x){
      # if x is between 0-5, mutate values greater than 0 to 1, equal to 0 to 0 and smaller than 0 to -1
      case_when(between(x,0,5) & x > 3 ~ 1, between(x,0,5) & x == 3 ~ 0, between(x,0,5) & x < 3 ~ -1,
        x == "Positive" ~ 1, x == "Negative" ~ -1, x == "Neutral" ~ 0)})
}

reviews_discrete <- get_discrete(reviews_sentiment, c("afinn","vader","lsd"), "rating")
twitter_discrete <- get_discrete(twitter_sentiment, c("afinn","vader","lsd"), "rating")
parlvote_discrete <- get_discrete(parlvote_sentiment, c("afinn","vader","lsd"), "rating")
amazon_discrete <- get_discrete(amazon_sentiment, c("afinn","vader","lsd"), "rating")
  
parlvote_binary

```
### Calculate Accuracy, Precision, Recall
```{r}
# Function to get statistics (accuracy, precision, recall) of data frame for specific lexicon
#
# Arg: 
#   df: data frame that we want statistics of
#   lexicon: binary/ternary lexicon that should be evaluated
#
# Returns:
#   overall accuracy score (first element in output list)
#   data frame with statistics (second element in output list)
  
get_statistics <- function(df, lexicon){
  
  # create confusion matrix (via caret library) to get statistics of data frame
  cm <- confusionMatrix(factor(df[[lexicon]]), factor(df$rating), mode="prec_recall")
  
  # retrieve accuracy
  acc <- (as.data.frame(cm$overall))["Accuracy",]
  # retrieve statistics
  stats <- as.data.frame(cm$byClass)
  results <- list(acc, stats)

  return(results)
}

reviews_afinn.acc <- get_statistics(reviews_binary, "afinn")[[1]][1]
reviews_afinn.stats <- get_statistics(reviews_binary, "afinn")[2]

reviews_lsd.acc <- get_statistics(reviews_binary, "lsd")[[1]][1]
reviews_lsd.stats <- get_statistics(reviews_binary, "lsd")[2]

reviews_vader.acc <- get_statistics(reviews_binary, "vader")[[1]][1]
reviews_vader.stats <- get_statistics(reviews_binary, "vader")[2]

reviews_afinn.stats
reviews_afinn.acc
```
# 4. Step: Plot Data 
```{r}
# plot columns
reviews_dfm <- melt(head(reviews_sentiment,50)[,c('id','afinn','lsd','vader')],id.vars = 1)

reviews_plot <- ggplot(reviews_dfm,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
                ggtitle("Reviews Sentiments")


twitter_dfm <- melt(head(twitter_sentiment,50)[,c('id','afinn','lsd','vader')],id.vars = 1)

twitter_plot <- ggplot(twitter_dfm,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
                ggtitle("Twitter Sentiments")

parlvote_dfm <- melt(head(parlvote_sentiment,50)[,c('id','afinn','lsd','vader')],id.vars = 1)

parlvote_plot <- ggplot(parlvote_dfm,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
                facet_wrap(~ variable, ncol = 1, scales="free_y")+
                ggtitle("ParlVote Sentiments")

#parlvote_norm_dfm <- melt(head(parlvote_sentiment,50)[,c('id','afinn_norm','lsd_norm','vader_norm')],id.vars = 1)

#parlvote_norm_plot <- ggplot(parlvote_norm_dfm,aes(x = id,y = value)) + 
              #  geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
               # facet_wrap(~ variable, ncol = 1, scales="free_y")+
                #ggtitle("ParlVote Sentiments")


reviews_plot
twitter_plot
parlvote_plot
#parlvote_norm_plot
```
### Plot important words
```{r}
# alternative sentiment function (can be used as final one), added get_tokens
get_sentiment_tokens2 <- function(df, lexicons, normalize = TRUE, get_tokens = TRUE){
   
   if(get_tokens==TRUE){
     df <- df %>%
       unnest_tokens(token,text)
     
     tok = df$token
     
     #print(tok)
   }else{
     tok = tokens(df$text)
   }
   # for each lexicon, get sentiment scores and save scores in new column of data frame
   for(lex in lexicons){
     
     if(lex == "afinn"){
     df$afinn <- round(textstat_valence(tok, afinn, normalize="dictionary")$sentiment,3)}
     
     if(lex == "lsd"){
     df$lsd<- round(textstat_polarity(tok, lsd)$sentiment,3)}
     
     if(lex == "vader"){
     df$vader <- round(vader_df(tok)$compound,3)}
   }
   
   # normalize sentiment scores if TRUE, VADER scores are already normalized
   if(normalize==TRUE){
     df$afinn <- round(normalize2(df$afinn), 3)
     df$lsd <- round(normalize2(df$lsd), 3)
   }
   
   # set sentiment scores to 0 if NA
   df[is.na(df)] <- 0
   
   return(df)
 }

reviews_tok_sent <- get_sentiment_tokens2(reviews, c("afinn","lsd", "vader"))
twitter_tok_sent <- get_sentiment_tokens2(twitter, c("afinn","lsd", "vader"))
parlvote_tok_sent <- get_sentiment_tokens2(parlvote, c("afinn","lsd", "vader"))
amazon_tok_sent <- get_sentiment_tokens2(amazon, c("afinn","lsd", "vader"))

# remove stopwords 
data(stop_words)
 
reviews_tok_discrete <- get_discrete(reviews_tok_sent, c("afinn","vader","lsd"), "rating")
 
# get word counts per discrete score ("positive", "negative", "neutral", i.e. 1,-1,0)
reviews_afinn.word_counts <- reviews_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

reviews_lsd.word_counts <- reviews_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

reviews_vader.word_counts <- reviews_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

# plot top n words for each sentiment 
#n = 20 
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
 
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
 
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")

topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
#reviews_vader.word_counts

#library("ggpubr")
 
#figure <- ggarrange(topn_reviews_afinn.plot, topn_reviews_lsd.plot,topn_reviews_vader.plot,
                    #labels = c("Afinn", "LSD", "VADER"),
                    #ncol = 1, nrow = 3)
#figure
```

```{r}
# important words as csv file for each corpus

amazon_tok_discrete <- get_discrete(amazon_tok_sent, c("afinn","vader","lsd"), "rating")
parlvote_tok_discrete <- get_discrete(parlvote_tok_sent, c("afinn","vader","lsd"), "rating")
twitter_tok_discrete <- get_discrete(twitter_tok_sent, c("afinn","vader","lsd"), "rating")
 

# get word counts per discrete score ("positive", "negative", "neutral", i.e. 1,-1,0)

### AMAZON
amazon_afinn.word_counts <- amazon_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

amazon_lsd.word_counts <- amazon_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

amazon_vader.word_counts <- amazon_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

### PARLVOTE

parlvote_afinn.word_counts <- parlvote_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

parlvote_lsd.word_counts <- parlvote_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

parlvote_vader.word_counts <- parlvote_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

### TWITTER

twitter_afinn.word_counts <- twitter_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

twitter_lsd.word_counts <- twitter_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

twitter_vader.word_counts <- twitter_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()


```
```{r}
topn_amazon_afinn.plot <- amazon_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (Afinn)")
 
topn_amazon_lsd.plot <- amazon_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (LSD)")
 
topn_amazon_vader.plot <- amazon_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (VADER)")

topn_amazon_afinn.plot
topn_amazon_lsd.plot
topn_amazon_vader.plot
```

```{r}
topn_parlvote_afinn.plot <- parlvote_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (Afinn)")
 
topn_parlvote_lsd.plot <- parlvote_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
        slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (LSD)")
 
topn_parlvote_vader.plot <- parlvote_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (VADER)")

topn_parlvote_afinn.plot
topn_parlvote_lsd.plot
topn_parlvote_vader.plot
```

```{r}
topn_twitter_afinn.plot <- twitter_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (Afinn)")
 
topn_twitter_lsd.plot <- twitter_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (LSD)")
 
topn_twitter_vader.plot <- twitter_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (VADER)")

topn_twitter_afinn.plot
topn_twitter_lsd.plot
topn_twitter_vader.plot
```
```{r}
twitter_vader.word_counts
```

```{r}
# write data to csv file 
### TWITTER
write.csv(twitter_vader.word_counts, "datasets/sentiment_data/twitter_vader_topn.csv", row.names = FALSE)
write.csv(twitter_lsd.word_counts, "datasets/sentiment_data/twitter_lsd_topn.csv", row.names = FALSE)
write.csv(twitter_afinn.word_counts, "datasets/sentiment_data/twitter_afinn_topn.csv", row.names = FALSE)

### AMAZON
write.csv(amazon_vader.word_counts, "datasets/sentiment_data/amazon_vader_topn.csv", row.names = FALSE)
write.csv(amazon_lsd.word_counts, "datasets/sentiment_data/amazon_lsd_topn.csv", row.names = FALSE)
write.csv(amazon_afinn.word_counts, "datasets/sentiment_data/amazon_afinn_topn.csv", row.names = FALSE)

### REVIEWS
write.csv(reviews_vader.word_counts, "datasets/sentiment_data/reviews_vader_topn.csv", row.names = FALSE)
write.csv(reviews_lsd.word_counts, "datasets/sentiment_data/reviews_lsd_topn.csv", row.names = FALSE)
write.csv(reviews_afinn.word_counts, "datasets/sentiment_data/reviews_afinn_topn.csv", row.names = FALSE)

### PARLVOTE
write.csv(parlvote_vader.word_counts, "datasets/sentiment_data/parlvote_vader_topn.csv", row.names = FALSE)
write.csv(parlvote_lsd.word_counts, "datasets/sentiment_data/parlvote_lsd_topn.csv", row.names = FALSE)
write.csv(parlvote_afinn.word_counts, "datasets/sentiment_data/parlvote_afinn_topn.csv", row.names = FALSE)
```

### Ranking of texts per lexicon
```{r}
# sort per corpus and per tool
reviews_afinn.sort <- reviews_sentiment[order(reviews_sentiment$afinn, decreasing=TRUE),]

reviews_lsd.sort <- reviews_sentiment[order(reviews_sentiment$lsd, decreasing=TRUE),]

reviews_vader.sort <- reviews_sentiment[order(reviews_sentiment$vader, decreasing=TRUE),]


reviews_afinn.sort
reviews_lsd.sort
reviews_vader.sort
```

# 5. Step: Evaluation (Notes)
- evaluate data/ compare data
- accuracy, precision, recall 
- pearsons coefficient 

Comparison Groups:
- compare binary (normalized) versions
  - each lexicon and data set
- compare continuous (normalized) versions
  - each lexicon and data set
- compare top n words per sentiment per tool? (= contribution to sentiment)
- compare ranking (Stede's idea):
  - rank texts per corpus and compare across tools -> is order similar?
- tool's performance
  - how is performance across domains?
  - is discrete format more accurate than continuous scoring?

