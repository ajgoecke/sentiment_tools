---
title: "Comparison of Sentiment Tools across Domains"
output:
  html_document:
    df_print: paged
---
```{r}
# load required libraries

# to use harry potter dataset
# devtools::install_github("bradleyboehmke/harrypotter")
# devtools::install_github("quanteda/quanteda.sentiment")
# devtools::install_github("quanteda/quanteda.corpora")

library(quanteda)
library(readtext)
library(corpus)
library(tidyverse)
library(stringr)
library(tidytext)
library(harrypotter)
library(dplyr)
library(quanteda.sentiment)
library(vader)
library(caret)
library(reshape2)


require(quanteda)
require(quanteda.corpora)
require(quanteda.sentiment)
#library("quanteda", warn.conflicts = FALSE, quietly = TRUE)
```

# 1. Step: Load Data & Lexicons
```{r}
# load BASELINE datasets
reviews <- readRDS(file="datasets/baseline/books_config_base.rds")
twitter <- readRDS(file="datasets/baseline/twitter_config_base.rds")
parlvote <- readRDS(file="datasets/baseline/parl_config_base.rds")
amazon <- readRDS(file="datasets/baseline/ama_config_base.rds")
finance <- readRDS(file="datasets/baseline/fin_config_base.rds")
parlvote_corr <- readRDS(file="datasets/corr_part.rds") 

twitter_bin <- readRDS(file="datasets/baseline/twitterbin_config_base.rds")
finance_bin <- readRDS(file="datasets/baseline/finbin_config_base.rds")

# load preprocessed datasets
### CONFIG2: lemmatization
reviews_config2 <- readRDS(file="datasets/config_2/book_config_2.rds")
twitter_config2 <- readRDS(file="datasets/config_2/twitter_config_2.rds")
parlvote_config2 <- readRDS(file="datasets/config_2/parl_config_2.rds")
amazon_config2 <- readRDS(file="datasets/config_2/ama_config_2.rds")
finance_config2 <- readRDS(file="datasets/config_2/fin_config_2.rds")

twitter_bin2 <- readRDS(file="datasets/config_2/twitterbin_config_2.rds")
finance_bin2 <- readRDS(file="datasets/config_2/finbin_config_2.rds")

### CONFIG1: lemmatization
reviews_config1 <- readRDS(file="datasets/config_1/book_config_1.rds")
twitter_config1 <- readRDS(file="datasets/config_1/twitter_config_1.rds")
parlvote_config1 <- readRDS(file="datasets/config_1/parl_config_1.rds")
amazon_config1 <- readRDS(file="datasets/config_1/ama_config_1.rds")
finance_config1 <- readRDS(file="datasets/config_1/fin_config_1.rds")

twitter_bin1 <- readRDS(file="datasets/config_1/twitterbin_config_1.rds")
finance_bin1 <- readRDS(file="datasets/config_1/finbin_config_1.rds")

### CONFIG3: lemmatization
reviews_config3 <- readRDS(file="datasets/config_3/book_config_3.rds")
twitter_config3 <- readRDS(file="datasets/config_3/twitter_config_3.rds")
parlvote_config3 <- readRDS(file="datasets/config_3/parl_config_3.rds")
amazon_config3 <- readRDS(file="datasets/config_3/ama_config_3.rds")
finance_config3 <- readRDS(file="datasets/config_3/fin_config_3.rds")

twitter_bin3 <- readRDS(file="datasets/config_3/twitterbin_config_3.rds")
finance_bin3 <- readRDS(file="datasets/config_3/finbin_config_3.rds")

# load lexicons
afinn <- data_dictionary_AFINN
lsd <- data_dictionary_LSD2015
```

```{r}
reviews
amazon
twitter
finance
parlvote
parlvote_corr
```

# 2. Step: Perform Sentiment Analysis
### Normalize Scores
```{r}
# DATA NORMALIZATION
# Normalize data via minimun/maximum normalization, either by scaling values from 0 to 1 or -1 to 1
# 
# Arg:
#   x: input values (e.g. column of data frame)
#   
# Returns: 
#   normalized data

# min/max normalization from -1 to 1
normalize <- function(x, na.rm = TRUE){
  return(2* ((x - min(x)) / (max(x)-min(x)))-1)}

# min/max normalization for afinn data 
normalize_afinn <- function(x, na.rm = TRUE){
  return(2* ((x - (-5)) / (5-(-5)))-1)}
```

### Calculate Sentiment Scores: OLD!!!
```{r}
# Calculate sentiment scores for different lexicons and input data frames
# 
# Arg:
#   data: input data frame
#   lexicons: names of lexicons that should be used for sentiment scoring
#   normalize: TRUE or FALSE, to optionally normalize sentiment scores
#   
# Returns: 
#   Data frame with (normalized) sentiment sores for chosen lexicons

get_sentiment <- function(df, lexicons, normalize = TRUE){
  
  # for each lexicon, get sentiment scores and save scores in new column of data frame
  for(lex in lexicons){
    
    if(lex == "afinn"){
    df$afinn <- round(textstat_valence(tokens(df$text), afinn, normalize="dictionary")$sentiment,3)}
    
    if(lex == "lsd"){
    df$lsd<- round(textstat_polarity(tokens(df$text), lsd, fun=sent_relpropdiff)$sentiment,3)}
    
    if(lex == "vader"){
    df$vader <- round(vader_df(df$text)$compound,3)}
  }
  
  # normalize sentiment scores if TRUE, VADER scores are already normalized
  if(normalize==TRUE){
    df$afinn <- round(normalize_afinn(df$afinn), 3)
    #df$lsd <- round(normalize2(df$lsd), 3)
  }
  
  # set sentiment scores to 0 if NA
  df[is.na(df)] <- 0
  
  return(df)
}

reviews_sentiment <- get_sentiment(reviews, c("afinn","lsd", "vader"))
twitter_sentiment <- get_sentiment(twitter, c("afinn","lsd", "vader"))
parlvote_sentiment <- get_sentiment(parlvote, c("afinn","lsd", "vader"))
amazon_sentiment <- get_sentiment(amazon, c("afinn", "lsd", "vader"))

# to save data frame as csv file
#write.csv(parlvote_sentiment, "datasets/parlvote_sentiment.csv", row.names = FALSE)

reviews_sentiment
```


#### Calculate Sentiment Scores: NEW!!!!
```{r}
# Calculate sentiment scores for different lexicons and input data frames
# 
# Arg:
#  data: input data frame
#  lexicons: names of lexicons that should be used for sentiment scoring
#  normalize: "relative" if normalization is handled relative to output, i.e. output column of afinn is being scaled via min/max normalization
#  normalize: "afinn" if normalization is handled by taking -5 as new minimum and +5 as new maximum and everything else is scaled between
#  get_tokens: if TRUE final data frame consists of single tokens with an associated sentiment score
#              else final data frame consists of an associated sentiment score per input text instead (only used to calculate TOP-N words)
#   
# Returns: 
#  data frame with (normalized) sentiment sores for chosen lexicons

get_sentiment <- function(df, lexicons, normalize, get_tokens){
   
   # if we want data frame with single tokens 
   if(get_tokens==TRUE){
      df <- df %>%
         # get list of tokens as new col in data frame
         unnest_tokens(token,text)
      
      # assign the new col as input for sentiment lexicons
      tok = df$token
     
      # for each lexicon, get sentiment scores and save scores in new column of data frame
      for(lex in lexicons){
         
         if(lex == "afinn"){
            df$afinn <- round(textstat_valence(tok, afinn, normalize="dictionary")$sentiment,3)}
         if(lex == "lsd"){
            df$lsd<- round(textstat_polarity(tok, lsd, fun=sent_relpropdiff)$sentiment,3)}
     
         if(lex == "vader"){
            df$vader <- round(vader_df(tok)$compound,3)}
   }
    
   # if we don't want to analyze single tokens but input text as "whole"
   }else{
      
      tok = tokens(df$text)
      
      for(lex in lexicons){
         if(lex == "afinn"){
            df$afinn <- round(textstat_valence(tok, afinn, normalize="dictionary")$sentiment,3)}
         
         if(lex == "lsd"){
            df$lsd<- round(textstat_polarity(tok, lsd, fun=sent_relpropdiff)$sentiment,3)}
         
         if(lex == "vader"){
            df$vader <- round(vader_df(df$text)$compound,3)}
     }
   }  
   
   # normalize sentiment scores if TRUE, VADER and LSD scores are already normalized within functions above

   if(normalize=="afinn"){
      df$afinn <- round(normalize_afinn(df$afinn), 3)
   }else{
      df$afinn <- round(normalize(df$afinn), 3)}
   
   # set sentiment scores to 0 if NA
   df[is.na(df)] <- 0
   
   return(df)
}
```

```{r}
### BASELINE  
reviews_sentiment_norm1 <- get_sentiment(reviews, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_sentiment_norm1 <- get_sentiment(twitter, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
parlvote_sentiment_norm1 <- get_sentiment(parlvote, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
amazon_sentiment_norm1 <- get_sentiment(amazon, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
finance_sentiment_norm1 <- get_sentiment(finance, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
parlvote_corr_norm1 <- get_sentiment(parlvote_corr, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

# binary version
finance_bin_norm1 <- get_sentiment(finance_bin, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_bin_norm1 <- get_sentiment(twitter_bin, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

### CONFIG2
reviews_config2_sentiment_norm1 <- get_sentiment(reviews_config2, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_config2_sentiment_norm1 <- get_sentiment(twitter_config2, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
parlvote_config2_sentiment_norm1 <- get_sentiment(parlvote_config2, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
amazon_config2_sentiment_norm1 <- get_sentiment(amazon_config2, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
finance_config2_sentiment_norm1 <- get_sentiment(finance_config2, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

# Binary Version
finance_bin2_norm1 <- get_sentiment(finance_bin2, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_bin2_norm1 <- get_sentiment(twitter_bin2, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

### CONFIG1
reviews_config1_sentiment_norm1 <- get_sentiment(reviews_config1, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_config1_sentiment_norm1 <- get_sentiment(twitter_config1, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
parlvote_config1_sentiment_norm1 <- get_sentiment(parlvote_config1, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
amazon_config1_sentiment_norm1 <- get_sentiment(amazon_config1, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
finance_config1_sentiment_norm1 <- get_sentiment(finance_config1, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

# Binary Version
finance_bin1_norm1 <- get_sentiment(finance_bin1, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_bin1_norm1 <- get_sentiment(twitter_bin1, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

### CONFIG3
reviews_config3_sentiment_norm1 <- get_sentiment(reviews_config3, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_config3_sentiment_norm1 <- get_sentiment(twitter_config3, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
parlvote_config3_sentiment_norm1 <- get_sentiment(parlvote_config3, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=FALSE)
amazon_config3_sentiment_norm1 <- get_sentiment(amazon_config3, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
finance_config3_sentiment_norm1 <- get_sentiment(finance_config3, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

# Binary Version
finance_bin3_norm1 <- get_sentiment(finance_bin3, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)
twitter_bin3_norm1 <- get_sentiment(twitter_bin3, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=FALSE)

```

```{r}
# write data to csv file 

### BASELINE
write.csv(parlvote_sentiment_norm1, "datasets/sentiment_scores/parlvote_sent_norm1.csv", row.names = FALSE)
write.csv(amazon_sentiment_norm1, "datasets/sentiment_scores/amazon_sent_norm1.csv", row.names = FALSE)
write.csv(twitter_sentiment_norm1, "datasets/sentiment_scores/twitter_sent_norm1.csv", row.names = FALSE)
write.csv(reviews_sentiment_norm1, "datasets/sentiment_scores/reviews_sent_norm1.csv", row.names = FALSE)
write.csv(finance_sentiment_norm1, "datasets/sentiment_scores/finance_sent_norm1.csv", row.names = FALSE)
write.csv(parlvote_corr_norm1, "datasets/sentiment_scores/parlvote_corr_norm1.csv", row.names = FALSE)
write.csv(finance_bin_norm1, "datasets/sentiment_scores/finance_bin_norm1.csv", row.names = FALSE)
write.csv(twitter_bin_norm1, "datasets/sentiment_scores/twitter_bin_norm1.csv", row.names = FALSE)

### CONFIG2
write.csv(parlvote_config2_sentiment_norm1, "datasets/sentiment_scores/parlvote_config2_sent_norm1.csv", row.names = FALSE)
write.csv(amazon_config2_sentiment_norm1, "datasets/sentiment_scores/amazon_config2_sent_norm1.csv", row.names = FALSE)
write.csv(twitter_config2_sentiment_norm1, "datasets/sentiment_scores/twitter_config2_sent_norm1.csv", row.names = FALSE)
write.csv(reviews_config2_sentiment_norm1, "datasets/sentiment_scores/reviews_config2_sent_norm1.csv", row.names = FALSE)
write.csv(finance_config2_sentiment_norm1, "datasets/sentiment_scores/finance_config2_sent_norm1.csv", row.names = FALSE)
write.csv(finance_bin2_norm1, "datasets/sentiment_scores/finance_bin2_norm1.csv", row.names = FALSE)
write.csv(twitter_bin2_norm1, "datasets/sentiment_scores/twitter_bin2_norm1.csv", row.names = FALSE)

### CONFIG1
write.csv(parlvote_config1_sentiment_norm1, "datasets/sentiment_scores/parlvote_config1_sent_norm1.csv", row.names = FALSE)
write.csv(amazon_config1_sentiment_norm1, "datasets/sentiment_scores/amazon_config1_sent_norm1.csv", row.names = FALSE)
write.csv(twitter_config1_sentiment_norm1, "datasets/sentiment_scores/twitter_config1_sent_norm1.csv", row.names = FALSE)
write.csv(reviews_config1_sentiment_norm1, "datasets/sentiment_scores/reviews_config1_sent_norm1.csv", row.names = FALSE)
write.csv(finance_config1_sentiment_norm1, "datasets/sentiment_scores/finance_config1_sent_norm1.csv", row.names = FALSE)
write.csv(finance_bin1_norm1, "datasets/sentiment_scores/finance_bin1_norm1.csv", row.names = FALSE)
write.csv(twitter_bin1_norm1, "datasets/sentiment_scores/twitter_bin1_norm1.csv", row.names = FALSE)

### CONFIG3
write.csv(parlvote_config3_sentiment_norm1, "datasets/sentiment_scores/parlvote_config3_sent_norm1.csv", row.names = FALSE)
write.csv(amazon_config3_sentiment_norm1, "datasets/sentiment_scores/amazon_config3_sent_norm1.csv", row.names = FALSE)
write.csv(twitter_config3_sentiment_norm1, "datasets/sentiment_scores/twitter_config3_sent_norm1.csv", row.names = FALSE)
write.csv(reviews_config3_sentiment_norm1, "datasets/sentiment_scores/reviews_config3_sent_norm1.csv", row.names = FALSE)
write.csv(finance_config3_sentiment_norm1, "datasets/sentiment_scores/finance_config3_sent_norm1.csv", row.names = FALSE)
write.csv(finance_bin3_norm1, "datasets/sentiment_scores/finance_bin3_norm1.csv", row.names = FALSE)
write.csv(twitter_bin3_norm1, "datasets/sentiment_scores/twitter_bin3_norm1.csv", row.names = FALSE)
```

# 3. Step: Convert Data
### Convert data into discrete format
```{r}
# Convert final values into ternary (1 = positive, 0 = neutral, -1 = negative) format for evaluation and comparison
# 
# Arg:
#   df: input data frame that contains the columns to be converted into ternary format
#   to_change: column names that should be converted into ternary format
#   gold: goldstandard, i.e. rating, which has to be changed to ternary format
#   
# Returns: 
#   data frame with converted values

get_discrete <- function(df, to_change, gold){
  df %>% 
    mutate_at(to_change, function(x){
      # mutate values greater than 0 to 1 (positive), equal to 0 to 0 (neutral) and smaller than 0 to -1 (negative)
      case_when(x > 0 ~ 1, x < 0 ~ -1, x == 0 ~ 0)})# %>% 
    
    #mutate_at(gold, function(x){
      # if x is between 0-5, mutate values greater than 0 to 1, equal to 0 to 0 and smaller than 0 to -1
      #case_when(between(x,0,5) & x > 3 ~ 1, 
                #between(x,0,5) & x == 3 ~ 0, 
                #between(x,0,5) & x < 3 ~ -1,
                #x == "Positive" | x=="positive" ~ 1,
                #x == "Negative" | x=="negative" ~ -1, 
                #x == "Neutral" | x=="neutral" ~ 0)})
}

reviews_discrete <- get_discrete(reviews_sentiment_norm1, c("afinn","vader","lsd"), "rating")
twitter_discrete <- get_discrete(twitter_sentiment_norm1, c("afinn","vader","lsd"), "rating")
parlvote_discrete <- get_discrete(parlvote_sentiment_norm1, c("afinn","vader","lsd"), "rating")
amazon_discrete <- get_discrete(amazon_sentiment_norm1, c("afinn","vader","lsd"), "rating")
finance_discrete <- get_discrete(finance_sentiment_norm1, c("afinn","vader","lsd"), "rating")
```










### OPTIONAL: Calculate Accuracy, Precision, Recall
```{r}
# Function to get statistics (accuracy, precision, recall) of data frame for specific lexicon
#
# Arg: 
#   df: data frame that we want statistics of
#   lexicon: binary/ternary lexicon that should be evaluated
#
# Returns:
#   overall accuracy score (first element in output list)
#   data frame with statistics (second element in output list)
  
get_statistics <- function(df, lexicon){
  
  # create confusion matrix (via caret library) to get statistics of data frame
  cm <- confusionMatrix(factor(df[[lexicon]]), factor(df$rating), mode="prec_recall")
  
  # retrieve accuracy
  acc <- (as.data.frame(cm$overall))["Accuracy",]
  # retrieve statistics
  stats <- as.data.frame(cm$byClass)
  results <- list(acc, stats)

  return(results)
}

#reviews_afinn.acc <- get_statistics(reviews_discrete, "afinn")[[1]][1]
#reviews_afinn.stats <- get_statistics(reviews_discrete, "afinn")[2]
#
#reviews_lsd.acc <- get_statistics(reviews_discrete, "lsd")[[1]][1]
#reviews_lsd.stats <- get_statistics(reviews_discrete, "lsd")[2]

#reviews_vader.acc <- get_statistics(reviews_discrete, "vader")[[1]][1]
#reviews_vader.stats <- get_statistics(reviews_discrete, "vader")[2]

#reviews_afinn.stats
#reviews_vader.acc

twitter_afinn.acc <- get_statistics(twitter_discrete, "afinn")[[1]][1]
twitter_afinn.stats <- get_statistics(twitter_discrete, "afinn")[2]

twitter_lsd.acc <- get_statistics(twitter_discrete, "lsd")[[1]][1]
twitter_lsd.stats <- get_statistics(twitter_discrete, "lsd")[2]

twitter_vader.acc <- get_statistics(twitter_discrete, "vader")[[1]][1]
twitter_vader.stats <- get_statistics(twitter_discrete, "vader")[2]

twitter_afinn.stats
twitter_vader.acc

twitter_discrete
```
# 4. Step: Plot Data 
```{r}
# ===== PLOT SENTIMENT SCORES =====

# create data frame with sentiment scores as variable of first 100 instances of corpus
reviews_df <- melt(head(reviews_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
twitter_df <- melt(head(twitter_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
parlvote_df <- melt(head(parlvote_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
amazon_df <- melt(head(amazon_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
finance_df <- melt(head(finance_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)

# create plots for each corpus
reviews_plot <- ggplot(reviews_df,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
                facet_wrap(~ variable, ncol = 1, scales="free_y")+
                xlab("text id")+ ylab("sentiment score")+
                ggtitle("Reviews Sentiment")

twitter_plot <- ggplot(twitter_df,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
                xlab("text id")+ ylab("sentiment score")+
                ggtitle("Twitter Sentiment")


parlvote_plot <- ggplot(parlvote_df,aes(x = id,y = value)) + 
                 geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
                 #facet_wrap(~ variable, ncol = 1, scales="free_y")+
                 xlab("text id")+ ylab("sentiment score")+
                 ggtitle("ParlVote Sentiment")

amazon_plot <- ggplot(amazon_df,aes(x = id,y = value)) + 
               geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
               #facet_wrap(~ variable, ncol = 1, scales="free_y")+
               xlab("text id")+ ylab("sentiment score")+
               ggtitle("Amazon Sentiment")


finance_plot <- ggplot(finance_df,aes(x = id,y = value)) + 
                geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
                #facet_wrap(~ variable, ncol = 1, scales="free_y")+
                xlab("text id")+ ylab("sentiment score")+
                ggtitle("Finance Sentiment")

reviews_plot.line <- ggplot(reviews_df,aes(x = id, y = value, group=variable)) +
                     geom_line(aes(colour=variable), size=0.4)+ 
                     ylim(-1,1) +
                     ggtitle("Reviews Sentiment Scores")

# show plots
reviews_plot
reviews_plot.line
twitter_plot
parlvote_plot
amazon_plot
finance_plot 

#ggsave("plot_ex.png", plot=plot, width = 50, height = 20, units = "cm")
```
```{r}


```

### Plot important words
```{r}
reviews_tok_sent <- get_sentiment(reviews, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=TRUE)
twitter_tok_sent <- get_sentiment(twitter, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=TRUE)
parlvote_tok_sent <- get_sentiment(parlvote, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=TRUE)
amazon_tok_sent <- get_sentiment(amazon, c("afinn","lsd", "vader"), normalize="afinn", get_tokens=TRUE)
finance_tok_sent <- get_sentiment(finance, c("afinn", "lsd", "vader"), normalize="afinn", get_tokens=TRUE)


# convert values to discrete format 
reviews_tok_discrete <- get_discrete(reviews_tok_sent, c("afinn","vader","lsd"), "rating")
twitter_tok_discrete <- get_discrete(twitter_tok_sent, c("afinn","vader","lsd"), "rating")
parlvote_tok_discrete <- get_discrete(parlvote_tok_sent, c("afinn","vader","lsd"), "rating")
amazon_tok_discrete <- get_discrete(amazon_tok_sent, c("afinn","vader","lsd"), "rating")
finance_tok_discrete <- get_discrete(finance_tok_sent, c("afinn","vader","lsd"), "rating")

# load stopwords 
data(stop_words)
```

```{r}
reviews_tok_sent

clean_reviews <- reviews_tok_sent %>%
  anti_join(stop_words, by=reviews_tok_sent$token)


reviews.word_counts <- reviews_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

nrow(reviews.word_counts)
reviews.word_counts
colSums(reviews.word_counts != 0)
colSums(reviews_tok_sent != 0)
reviews_tok_sent
```


### Compute Coverage of Sentiment Lexicons
```{r}
# ===== COVERAGE PER TOKEN =====

# Coverage per token: if sentiment score is not zero, count as "covered", get percentage of covered tokens
reviews_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(reviews_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok_sent != 0)[[1]]*100,2))), "reviews")
twitter_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(twitter_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok_sent != 0)[[1]]*100,2))), "twitter")
parlvote_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok_sent != 0)[[1]]*100,2))), "parlvote")
amazon_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(amazon_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok_sent != 0)[[1]]*100,2))), "amazon")
finance_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(finance_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(finance_tok_sent != 0)[[1]]*100,2))), "finance")

# Coverage per token: with stopword removal 

# remove stopwords from each data frame 
reviews_tok.stopwords <- reviews_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

twitter_tok.stopwords <- twitter_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

parlvote_tok.stopwords <- parlvote_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

amazon_tok.stopwords <- amazon_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

finance_tok.stopwords <- finance_tok_sent %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

# same as above: if sentiment score is not zero, count as "covered", get percentage of covered tokens
reviews_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(reviews_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok.stopwords != 0)[[1]]*100,2))), "reviews_stopwords")
twitter_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(twitter_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok.stopwords != 0)[[1]]*100,2))), "twitter_stopwords")
parlvote_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok.stopwords != 0)[[1]]*100,2))), "parlvote_stopwords")
amazon_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(amazon_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok.stopwords != 0)[[1]]*100,2))), "amazon_stopwords")
finance_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(finance_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(finance_tok.stopwords != 0)[[1]]*100,2))), "finance_stopwords")

# save data to data frame
coverage_per_tok <- rbind(reviews_tok_coverage, twitter_tok_coverage,parlvote_tok_coverage, amazon_tok_coverage, finance_tok_coverage)
#write.csv(coverage_per_tok.stopwords, "datasets/coverage/coverage_per_tok_stopwords.csv", row.names = TRUE)

coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords,twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)

# ===== COVERAGE PER TEXT =====

# Coverage per text: if sentiment score is not zero, count as "covered", get percentage of covered text instances
reviews_coverage <- `rownames<-`(data.frame(t(colSums(reviews_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "reviews")
twitter_coverage <- `rownames<-`(data.frame(t(colSums(twitter_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "twitter")
parlvote_coverage <- `rownames<-`(data.frame(t(colSums(parlvote_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "parlvote")
amazon_coverage <- `rownames<-`(data.frame(t(colSums(amazon_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "amazon")
finance_coverage <- `rownames<-`(data.frame(t(colSums(finance_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "finance")

# save data to data frame 
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
#write.csv(coverage_per_text, "datasets/coverage/coverage_per_text.csv", row.names = TRUE)

coverage <- rbind(reviews_tok_coverage,reviews_tok_coverage.stopwords,reviews_coverage, twitter_tok_coverage,twitter_tok_coverage.stopwords,parlvote_tok_coverage,twitter_coverage,parlvote_tok_coverage.stopwords, parlvote_coverage, amazon_tok_coverage,amazon_tok_coverage.stopwords, amazon_coverage, finance_tok_coverage,finance_tok_coverage.stopwords, finance_coverage)
```

```{r}
# add corpus names as rownames
coverage_per_tok["corpus"] <- rownames(coverage_per_tok)
coverage_per_tok.stopwords["corpus"] <- rownames(coverage_per_tok.stopwords)
coverage_per_text["corpus"] <- rownames(coverage_per_text)



# prepare data for plotting
cov_tok_df <- melt(coverage_per_tok,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_tok_df.stopwords <- melt(coverage_per_tok.stopwords,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_text_df <- melt(coverage_per_text,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")



# create plots
cov_tok_plot <- ggplot(cov_tok_df,aes(x=corpus, y = coverage)) + 
                geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
                xlab("Corpus")+ ylab("Coverage in %")+
                ggtitle("Coverage per Token")

cov_tok_plot.stopwords <- ggplot(cov_tok_df.stopwords,aes(x=corpus, y = coverage)) + 
                geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
                xlab("Corpus")+ ylab("Coverage in %")+
                ggtitle("Coverage per Token - Stopwords")

cov_text_plot <- ggplot(cov_text_df,aes(x=corpus, y = coverage)) + 
                geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
                xlab("Corpus")+ ylab("Coverage in %")+
                ggtitle("Coverage per Text")

cov_tok_plot
cov_tok_plot.stopwords
cov_text_plot

```
```{r}
# get word counts per discrete score ("positive", "negative", "neutral", i.e. 1,-1,0)
reviews_afinn.word_counts <- reviews_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

reviews_lsd.word_counts <- reviews_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

reviews_vader.word_counts <- reviews_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
 
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
 
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")

topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
```


```{r}
# get word counts per discrete score ("positive", "negative", "neutral", i.e. 1,-1,0)
finance_afinn.word_counts <- finance_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

finance_lsd.word_counts <- finance_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

finance_vader.word_counts <- finance_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

# plot top n words for each sentiment and for each lexicon, n = 20
topn_finance_afinn.plot <- finance_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Finance: Top 20 Words per Sentiment (Afinn)")
 
topn_finance_lsd.plot <- finance_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Finance: Top 20 Words per Sentiment (LSD)")
 
topn_finance_vader.plot <- finance_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Finance: Top 20 Words per Sentiment (VADER)")

topn_finance_afinn.plot
topn_finance_lsd.plot
topn_finance_vader.plot
```


```{r}
# important words as csv file for each corpus

amazon_tok_discrete <- get_discrete(amazon_tok_sent, c("afinn","vader","lsd"), "rating")
parlvote_tok_discrete <- get_discrete(parlvote_tok_sent, c("afinn","vader","lsd"), "rating")
twitter_tok_discrete <- get_discrete(twitter_tok_sent, c("afinn","vader","lsd"), "rating")
 

# get word counts per discrete score ("positive", "negative", "neutral", i.e. 1,-1,0)

### AMAZON
amazon_afinn.word_counts <- amazon_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

amazon_lsd.word_counts <- amazon_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

amazon_vader.word_counts <- amazon_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

### PARLVOTE
parlvote_afinn.word_counts <- parlvote_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

parlvote_lsd.word_counts <- parlvote_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

parlvote_vader.word_counts <- parlvote_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

### TWITTER
twitter_afinn.word_counts <- twitter_tok_discrete %>%
   count(token, afinn, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

twitter_lsd.word_counts <- twitter_tok_discrete %>%
   count(token, lsd, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()

twitter_vader.word_counts <- twitter_tok_discrete %>%
   count(token, vader, sort = TRUE) %>%
   anti_join(stop_words, by= c("token" = "word") ) %>%
   ungroup()
```

```{r}
# plot top n words for each sentiment and for each lexicon, n = 20
topn_amazon_afinn.plot <- amazon_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (Afinn)")
 
topn_amazon_lsd.plot <- amazon_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (LSD)")
 
topn_amazon_vader.plot <- amazon_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Amazon: Top 20 Words per Sentiment (VADER)")

topn_amazon_afinn.plot
topn_amazon_lsd.plot
topn_amazon_vader.plot
```

```{r}
# plot top n words for each sentiment and for each lexicon, n = 20
topn_parlvote_afinn.plot <- parlvote_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (Afinn)")
 
topn_parlvote_lsd.plot <- parlvote_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
        slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (LSD)")
 
topn_parlvote_vader.plot <- parlvote_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("ParlVote: Top 20 Words per Sentiment (VADER)")

topn_parlvote_afinn.plot
topn_parlvote_lsd.plot
topn_parlvote_vader.plot
```

```{r}
topn_twitter_afinn.plot <- twitter_afinn.word_counts %>%
         group_by(afinn) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = afinn)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~afinn, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (Afinn)")
 
topn_twitter_lsd.plot <- twitter_lsd.word_counts %>%
         group_by(lsd) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = lsd)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~lsd, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (LSD)")
 
topn_twitter_vader.plot <- twitter_vader.word_counts %>%
         group_by(vader) %>%
         #top_n(20) %>%
         slice(1:20) %>%
         ggplot(aes(reorder(token, n), n, fill = vader)) +
           geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
           facet_wrap(~vader, scales = "free_y") +
           labs(y = "contribution to sentiment", x = NULL) +
           coord_flip()+
           ggtitle("Twitter: Top 20 Words per Sentiment (VADER)")

topn_twitter_afinn.plot
topn_twitter_lsd.plot
topn_twitter_vader.plot
```
```{r}
# write data to csv file 
### TWITTER
write.csv(twitter_vader.word_counts, "datasets/word_counts/twitter_vader_topn.csv", row.names = FALSE)
write.csv(twitter_lsd.word_counts, "datasets/word_counts/twitter_lsd_topn.csv", row.names = FALSE)
write.csv(twitter_afinn.word_counts, "datasets/word_counts/twitter_afinn_topn.csv", row.names = FALSE)

### AMAZON
write.csv(amazon_vader.word_counts, "datasets/word_counts/amazon_vader_topn.csv", row.names = FALSE)
write.csv(amazon_lsd.word_counts, "datasets/word_counts/amazon_lsd_topn.csv", row.names = FALSE)
write.csv(amazon_afinn.word_counts, "datasets/word_counts/amazon_afinn_topn.csv", row.names = FALSE)

### REVIEWS
write.csv(reviews_vader.word_counts, "datasets/word_counts/reviews_vader_topn.csv", row.names = FALSE)
write.csv(reviews_lsd.word_counts, "datasets/word_counts/reviews_lsd_topn.csv", row.names = FALSE)
write.csv(reviews_afinn.word_counts, "datasets/word_counts/reviews_afinn_topn.csv", row.names = FALSE)

### PARLVOTE
write.csv(parlvote_vader.word_counts, "datasets/word_counts/parlvote_vader_topn.csv", row.names = FALSE)
write.csv(parlvote_lsd.word_counts, "datasets/word_counts/parlvote_lsd_topn.csv", row.names = FALSE)
write.csv(parlvote_afinn.word_counts, "datasets/word_counts/parlvote_afinn_topn.csv", row.names = FALSE)

### FINANCE
write.csv(finance_vader.word_counts, "datasets/word_counts/finance_vader_topn.csv", row.names = FALSE)
write.csv(finance_lsd.word_counts, "datasets/word_counts/finance_lsd_topn.csv", row.names = FALSE)
write.csv(finance_afinn.word_counts, "datasets/word_counts/finance_afinn_topn.csv", row.names = FALSE)
```

### Ranking of texts per lexicon
```{r}
# sort per corpus and per tool
reviews_afinn.sort <- reviews_sentiment[order(reviews_sentiment$afinn, decreasing=TRUE),]

reviews_lsd.sort <- reviews_sentiment[order(reviews_sentiment$lsd, decreasing=TRUE),]

reviews_vader.sort <- reviews_sentiment[order(reviews_sentiment$vader, decreasing=TRUE),]


reviews_afinn.sort
reviews_lsd.sort
reviews_vader.sort
```

# 5. Step: Evaluation (Notes)
- evaluate data/ compare data
- accuracy, precision, recall 
- pearsons coefficient 

Comparison Groups:
- compare binary (normalized) versions
  - each lexicon and data set
- compare continuous (normalized) versions
  - each lexicon and data set
- compare top n words per sentiment per tool? (= contribution to sentiment)
- compare ranking (Stede's idea):
  - rank texts per corpus and compare across tools -> is order similar?
- tool's performance
  - how is performance across domains?
  - is discrete format more accurate than continuous scoring?

