---
title: "Preprocessing the data sets"
---

```{r}
#install.packages("tidyverse")

library(readtext)
library(rjson)
library(jsonlite)
library(tidyverse)
library(dplyr)
library(tidyverse)
library(quanteda)
```

ParlVote Corpus
```{r}
#Load ParlVote Data Set
#https://data.mendeley.com/datasets/czjfwgs9tm/1

ParlVote_concat <- read.csv("~/Studium/3 Semester/PM/Corpora/ParlVote_concat.csv")
```


```{r}
#only use relevant columns and throw out entries with non ascii characters
red_parl_vote<- subset(ParlVote_concat, select = c("speech","vote"))
red_parl_vote <-red_parl_vote[which(!grepl("[^\x01-\x7F]+",red_parl_vote$speech)),]

#Rename columns to standard column name
red_parl_vote<-red_parl_vote %>% 
 rename(
    text = speech,
    rating = vote)

#select a random sample of 1000 entries
red_parl_vote<-red_parl_vote[sample(nrow(red_parl_vote), 1000), ]

#add ID column
red_parl_vote$id <- 1:nrow(red_parl_vote) 
red_parl_vote<-red_parl_vote[,c(3,1,2)]

```

Book Review Corpus

```{r}
#Load Corpus
#https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home

setwd("/Users/rike-/Documents/Studium/3 Semester/PM/sentiment_tools-main/sentiment_tools-main/implementation")

reviews <- readtext("goodreads_reviews_children_2.json", text_field = "review_text")

```

```{r}
#add language tags to corpus (takes time)

#reviews$language <- textcat(reviews$text)
#saveRDS(reviews, file = "reviews_with_language.rds")

#or load corpus with languages already added
filename <- file.choose()
reviews <- readRDS(filename)

```


```{r}

#filter out any non english reviews
reviews[reviews$language== "english",]

red_review <- data.frame(text=character(),
                 rating=integer(), 
                 stringsAsFactors=FALSE) 

#filter for only rating and text columns and make equal distribution

for (i in 0:5){

hold<- subset(reviews, rating == i, select = c("text","rating"))

red_review <- rbind(red_review, hold[sample(nrow(hold), 167), ])
}


#throw out the last two entries to get exactly 1000 entries
red_review<- red_review[-c(1001,1002), ]

red_review<-as.data.frame(red_review)

#add an ID
red_review$id <- 1:nrow(red_review) 
red_review<-red_review[,c(3,1,2)]
 
red_review%>%
  count(rating)

```


Twitter Corpus
```{r}
#load corpus
#https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment

twitter <- read.csv("~/Studium/3 Semester/PM/Corpora/Sentiment.csv", comment.char="#")
```

```{r}
#filter only text and rating column and take only entries with a confidence over 0.6
red_twitter<- subset(twitter,sentiment_confidence <0.6, select = c("text","sentiment"))

#remove entries with non ASCII characters
red_twitter <-red_twitter[which(!grepl("[^\x01-\x7F]+",red_twitter$text)),]

#rename columns 
red_twitter<-red_twitter%>% 
 rename(rating = sentiment)

#filter a subset of 1000 entries
red_twitter<-red_twitter[sample(nrow(red_twitter), 1000), ]

#add ID
red_twitter$id <- 1:nrow(red_twitter) 
red_twitter<-red_twitter[,c(3,1,2)]

#show rating distribution
red_twitter%>%
  count(rating)
```

Amazon Product Reviews
```{r}
#Load partial Corpus 
#https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews
ama <- read.csv("C:/Users/rike-/Desktop/Industrial_and_Scientific.json.gz", header=FALSE,nrows=90000)
```
```{r}
am <- ama[grep("overall", ama$V1),]
```


```{r}
#The data structure got messed up for some entries Take only that have the key words "overall" and "review Text" in the right columns to ensure the correct data structure
ama_rev <- ama[grep("overall", ama$V1),]
ama_rev <- ama_rev[grep("reviewText", ama_rev$V7),]
#filter out any entries that but all information in one column
ama_rev<-dplyr::filter(ama_rev, !grepl("reviewerName",V7))

#make subset of only rating and text
ama_rev<- subset(ama_rev, select = c("V1","V7"))

#change name
ama_rev<-ama_rev%>% 
 rename(rating = V1,text=V7)

#throw the key word "reviewText" from the rating
ama_rev[,"text"]<-gsub("reviewText:", "",ama_rev[,"text"])

#transform rating into integer
ama_rev$rating[ama_rev$rating == "{overall: 5.0"] <- 5
ama_rev$rating[ama_rev$rating == "{overall: 4.0"] <- 4
ama_rev$rating[ama_rev$rating == "{overall: 3.0"] <- 3
ama_rev$rating[ama_rev$rating == "{overall: 2.0"] <- 2
ama_rev$rating[ama_rev$rating == "{overall: 1.0"] <- 1
ama_rev<-transform(ama_rev, rating = as.numeric(rating))

#remove NA
ama_rev<-ama_rev[!is.na(ama_rev$rating),]


red_ama <- data.frame(id=integer(),
                 text=character(),
                 rating=integer(), 
                 stringsAsFactors=FALSE) 

#make new dataframe with 1000 entires and equal distribution
for (i in 1:5){

hold<- subset(ama_rev, rating == i, select = c("text","rating"))

red_ama <- rbind(red_ama, hold[sample(nrow(hold), 200), ])
}

#add ID
red_ama$id <- 1:nrow(red_ama) 
red_ama<-red_ama[,c(3,1,2)]


red_ama%>%
  count(rating)
#ama_rev%>%
 # count(rating)
```

Kyoto Corpus (?)
```{r}
setwd("/Users/rike-/Documents/Studium/3 Semester/PM/kyoto texts/kyoto texts/")
#read in kyoto table
kyoto <- read.csv('kyoto_editorials_all.csv')
```


```{r}
#if needed save the reduced Dataframes
#saveRDS(red_twitter, file = "red_twitter.rds")
#saveRDS(red_parl_vote, file = "red_parl_vote.rds")
#saveRDS(red_review, file = "red_review.rds")
#saveRDS(red_ama, file = "red_ama.rds")
```




