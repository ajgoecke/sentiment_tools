---
title: "Comparison of Sentiment Tools across Domains"
output: html_notebook
---

Hint:
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
# load required libraries

# to use harry potter dataset
# devtools::install_github("bradleyboehmke/harrypotter")
# devtools::install_github("quanteda/quanteda.sentiment")
# devtools::install_github("quanteda/quanteda.corpora")



library(quanteda)
library(readtext)
library(corpus)
library(tidyverse)
library(stringr)
library(tidytext)
library(harrypotter)
#library(janeaustenr)
library(dplyr)
library(quanteda.sentiment)
library(vader)


require(quanteda)
require(quanteda.corpora)
require(quanteda.sentiment)

```

```{r}
# load afinn lexicon, first option

# manually -> convert to binary lexicon
afinn_dict <- read.csv("lexika/AFINN-111.txt", header = F, sep = "\t", stringsAsFactors = F)
afinn_binary <- dictionary(list(positive = afinn_dict$V1[afinn_dict$V2>0], negative = afinn_dict$V1[afinn_dict$V2<0]))

# provided via tidytext?
afinn <- get_sentiments("afinn")

```

# Harry Potter - Dataset
```{r}
# load harry potter dataset 
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")

books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
           goblet_of_fire, order_of_the_phoenix, half_blood_prince,
           deathly_hallows)
  
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             #unnest_tokens(word, text) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}

series$book <- factor(series$book, levels = rev(titles))

series
#book_groups <- series %>% group_by(book, chapter)
```


### Harry Potter - AFINN Lexicon
```{r}
#afinn_hp1 <- series %>%
 #       group_by(book) %>% 
  #      mutate(word_count = 1:n(),
   #            index = word_count %/% 500 + 1) %>% 
    #    inner_join(get_sentiments("afinn")) %>%
     #   group_by(book, index, chapter) %>%
      #  summarise(sentiment = sum(value)) %>%
       # mutate(method = "AFINN")

#afinn_hp1
```

```{r}
afinn_hp2 <- series %>%
        group_by(book, chapter) %>% # add word for single word scores 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, chapter) %>% # add word for single word scores
        #summarise(sentiment = sum(value)) %>%
        summarise(sentiment = mean(value, na.rm = TRUE)) %>%
        mutate(method = "AFINN")  %>%
        ggplot(aes(chapter, sentiment, fill = book)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x")

afinn_hp2

#ggsave(plot = afinn, width = 15, height = 15, dpi = 300, filename = "afinn_hp_mean.png")
```

### load afinn via quanteda.sentiment
```{r}
afinn2 <- data_dictionary_AFINN

afinn2


#data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]
#data_dictionary_LSD2015
```

```{r}
#library("quanteda", warn.conflicts = FALSE, quietly = TRUE)
#print(data_dictionary_AFINN, max_nval = 8)
```

# Lexicoder Sentiment Dictionary

```{r}
# tokenize hp1
hp1_tokenized <- tokens_tolower(tokens(philosophers_stone, remove_punct = TRUE)) 
```

```{r}
# select only the "negative" and "positive" categories
#data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]

#hp1_lsd <- tokens_lookup(hp1_tokenized, dictionary = data_dictionary_LSD2015_pos_neg)

polarity(data_dictionary_LSD2015) <- 
  list(pos = c("positive", "neg_negative"), neg = c("negative", "neg_positive"))

hp1_lsd <- textstat_polarity(hp1_tokenized, data_dictionary_LSD2015)

hp1_lsd_tokens <- tokens_lookup(hp1_tokenized, data_dictionary_LSD2015, nested_scope = "dictionary", exclusive = FALSE)

hp1_lsd
```

```{r}

hp1_lsd.df <- as.data.frame.matrix(hp1_lsd)

hp1_lsd.df$chapter <- 1:nrow(hp1_lsd.df)

plot <- ggplot(hp1_lsd, aes(x =hp1_lsd.df$chapter, y=sentiment)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE)
plot + ylim(-1.0, 1.0) + labs(y="sentiment", x = "chapter") + ggtitle("HP1 - Lexicoder")

#hp1_lsd.df

```

```{r}
hp1_afinn2 <- textstat_valence(hp1_tokenized, afinn2, normalize="dictionary")

hp1_afinn2.df <- as.data.frame.matrix(hp1_afinn2)

hp1_afinn2.df$chapter <- 1:nrow(hp1_afinn2.df)

plot <- ggplot(hp1_afinn2.df, aes(x =hp1_afinn2.df$chapter, y=sentiment)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE)
plot + ylim(-1.0, 1.0) + labs(y="sentiment", x = "chapter") + ggtitle("HP1 - AFINN")


#hp1_afinn2
```

# VADER

```{r}
get_vader(philosophers_stone[1])

hp1_vader <- vader_df(philosophers_stone)

hp1_vader


hp1_vader$chapter <- 1:nrow(hp1_vader)

plot <- ggplot(hp1_vader, aes(x =chapter, y=compound)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE)
plot + ylim(-5.0, 5.0) + labs(y="sentiment", x = "chapter") + ggtitle("HP1 - VADER")
```

```{r}
#library("quanteda", warn.conflicts = FALSE, verbose = FALSE)
#library("quanteda.sentiment", warn.conflicts = FALSE, verbose = FALSE)

#print(data_dictionary_LSD2015, max_nval = 5)
#lengths(data_dictionary_LSD2015)
```

# quanteda.sentiment: AFINN
```{r}
# Work with quanteda.sentiment on HP corpus:

# convert tibble to dataframe
series.df <- as.data.frame(series)

# tokenize books
series_tokenized <- series.df %>%
  unnest_tokens(tokens, text)

# apply afinn lexicon
series_tokenized$afinn2 <- textstat_valence(series_tokenized$tokens, afinn2)$sentiment

# replace all 0 values with na
series_tokenized[series_tokenized == 0] <- NA

series_tokenized %>%
  group_by(book, chapter) %>% # group df by book and chapter to get sentiment per chapter
  summarise(sentiment = mean(afinn2, na.rm = TRUE)) %>% # calculate mean w/o regarding na values
  mutate(method = "AFINN") %>% # add column with method 
        ggplot(aes(chapter, sentiment, fill = book)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x") +
          ggtitle("AFINN HP")
```

# quanteda.sentiment: Lexicoder 
```{r}
# Work with quanteda.sentiment on HP corpus:

# apply lexicoder lexicon
series$lsd <- textstat_polarity(tokens(series$text), data_dictionary_LSD2015)$sentiment 

#series.df <- as.data.frame(series)

plot <- ggplot(series, aes(chapter, lsd, fill = book)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x") +
          ggtitle("Lexicoder HP")
plot 
```

# Vader
```{r}
get_vader(philosophers_stone[1])

hp1_vader <- vader_df(philosophers_stone)
```

```{r}
hp1_vader$compound

hp1_vader$chapter <- 1:nrow(hp1_vader)

plot <- ggplot(hp1_vader, aes(x =chapter, y=compound)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE)
plot + ylim(-5.0, 5.0) + labs(y="sentiment", x = "chapter") + ggtitle("HP1 - VADER")
```
```{r}
# apply vader lexicon to all HP books
series$vader <- vader_df(series$text)$compound
```


```{r}
#series.df <- as.data.frame(series)

plot <- ggplot(series, aes(chapter, lsd, fill = book)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x") +
          ggtitle("VADER HP")
plot 
```

# REVIEWS DATASET
```{r}
# load dataset
reviews <- readtext("datasets/goodreads_reviews_children_2.json", text_field = "review_text")

# convert to dataframe
reviews.df <- as.data.frame(reviews)

# add doc_id (i.e. according to index)
reviews.df$doc_id <- 1:nrow(reviews.df)

reviews.df
```

```{r}
# get random sample of 50 reviews 
reviews_sample <- reviews.df[sample(1:nrow(reviews.df), 50,
   replace=FALSE),]

# get first 50 rows of data 
reviews_50 <- head(reviews.df,50)
reviews_50 = subset(reviews_50, select = c(doc_id,text,rating))
reviews_50
```

# Get Translations of Dataset 
```{r}
# either via corpus 
reviews.corpus <- corpus(reviews)
docvars(reviews.corpus, "language") <- textcat(reviews.corpus)
reviews_en <- corpus_subset(reviews.corpus, language == "english", drop_docid = TRUE)

# or via dataframe logic
reviews.df$language <- textcat(reviews.df$text)

reviews.df
```

# Perform SA on Reviews

### AFINN
```{r}
# Afinn
# tokenize 
reviews_tokenized <- reviews_50 %>%
  unnest_tokens(tokens, text)

# apply afinn lexicon
reviews_tokenized$afinn2 <- textstat_valence(reviews_tokenized$tokens, afinn2)$sentiment

# replace all 0 values with na
reviews_tokenized[reviews_tokenized == 0] <- NA

# calculate mean scores for tokens per doc
afinn_scores <- reviews_tokenized %>%
  group_by(doc_id) %>% # group df by doc_id to get mean sentiment score
  summarise(total = mean(afinn2, na.rm = TRUE)) #%>% # calculate mean w/o regarding na values

# add afinn scores to df 
reviews_50$afinn <- afinn_scores$total

# different version to get plot 
reviews_tokenized %>%
  group_by(doc_id) %>% # group df by book and chapter to get sentiment per chapter
  #reviews_tokenized$sentiment = mean(afinn2, na.rm = TRUE) %>%
  summarise(sentiment = mean(afinn2, na.rm = TRUE)) %>% # calculate mean w/o regarding na values
  mutate(method = "AFINN") %>% # add column with method 
        ggplot(aes(doc_id, sentiment, fill = doc_id)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          #facet_wrap(~ doc_id, ncol = 2, scales = "free_x") +
          ggtitle("AFINN Reviews")
```
```{r}
reviews_50
```

### LEXICODER
```{r}
# apply lexicoder lexicon
reviews_50$lsd <- textstat_polarity(tokens(reviews_50$text), data_dictionary_LSD2015)$sentiment 
#series.df <- as.data.frame(series)

plot <- ggplot(reviews_50, aes(doc_id, lsd, fill = doc_id)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          #facet_wrap(~ doc_id, ncol = 2, scales = "free_x") +
          ggtitle("Lexicoder Reviews")
plot 
```

```{r}
reviews_50$vader <- vader_df(reviews_50$text)$compound


plot <- ggplot(reviews_50, aes(doc_id, vader, fill = doc_id)) + # plot sentiment of books
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          #facet_wrap(~ doc_id, ncol = 2, scales = "free_x") +
          ggtitle("Vader Reviews")
plot 
```

# Check Golstandard
```{r}
# convert to binary results
reviews_50$afinn_binary[reviews_50$afinn > 0] <- "pos"
reviews_50$afinn_binary[reviews_50$afinn <= 0] <- "neg"

reviews_50$lsd_binary[reviews_50$lsd > 0] <- "pos"
reviews_50$lsd_binary[reviews_50$lsd <= 0] <- "neg"

reviews_50$vader_binary[reviews_50$vader > 0] <- "pos"
reviews_50$vader_binary[reviews_50$vader <= 0] <- "neg"

reviews_50$rating_binary[reviews_50$rating >= 3] <- "pos"
reviews_50$rating_binary[reviews_50$rating < 3] <- "neg"

#reviews_50[reviews_50 == NA] <- "neu"
```

```{r}
afinn_goldst <- sum(ifelse(reviews_50$rating_binary==reviews_50$afinn_binary,1,0), na.rm = TRUE)

lsd_goldst <- sum(ifelse(reviews_50$rating_binary==reviews_50$lsd_binary,1,0), na.rm = TRUE)

vader_goldst <- sum(ifelse(reviews_50$rating_binary==reviews_50$vader_binary,1,0), na.rm = TRUE)

lexicon <- c("afinn", "lexicoder", "vader")
sum <- c(afinn_goldst,lsd_goldst,vader_goldst)
accuracy <- c(afinn_goldst/nrow(reviews_50),lsd_goldst/nrow(reviews_50),vader_goldst/nrow(reviews_50))

final.df <- data.frame(lexicon, sum, accuracy)

```

```{r}
final.df
```

```{r}
# calculate precision and recall
#https://www.projectpro.io/recipes/calculate-precision-recall-and-f1-score-r
```

```{r}
test <- reviews_50

# 0 = negative, 1 = positive

# convert to binary results
test$afinn_binary[test$afinn > 0] <- 1
test$afinn_binary[test$afinn <= 0] <- 0

test$lsd_binary[test$lsd > 0] <- 1
test$lsd_binary[test$lsd <= 0] <- 0

test$vader_binary[test$vader > 0] <- 1
test$vader_binary[test$vader <= 0] <- 0

test$rating_binary[test$rating >= 3] <- 1
test$rating_binary[test$rating < 3] <- 0


test
```
```{r}
actual_values <- test$rating_binary
predict_values <- test$afinn_binary

# create confusion matrix 
confusion_matrix <- table(ACTUAL=actual_values, PREDICTED=predict_values)

# assign values from matrix to true/false positives/negatives
TN <- confusion_matrix[1]
FN <- confusion_matrix[2]
FP <- confusion_matrix[3]
TP <- confusion_matrix[4]

# calculate statistics
precision <- TP/(TP+FP)
accuracy <- (TP+TN)/(TP+TN+FP+FN)
recall <- TP/(TP+FN)
F1 <- (2*precision*recall)/(precision+recall)
```

```{r}
# function to get statistics

statistics <- data.frame(matrix(ncol=4, nrow=3))
x <- c("accuracy", "precision", "recall", "F1")
y <- c("afinn", "lsd", "vader")
rownames(statistics) <- y
colnames(statistics) <- x

get_statistics <- function(lexicon, actual_values, predict_values) {
  confusion_matrix <- table(ACTUAL=actual_values, PREDICTED=predict_values)
  TN <- confusion_matrix[1]
  FN <- confusion_matrix[2]
  FP <- confusion_matrix[3]
  TP <- confusion_matrix[4]
  
  # calculate statistics
  precision <- TP/(TP+FP)
  accuracy <- (TP+TN)/(TP+TN+FP+FN)
  recall <- TP/(TP+FN)
  F1 <- (2*precision*recall)/(precision+recall)
  
  output <- c(accuracy,precision, recall, F1)
  name <- deparse(substitute(lexicon))

  
  statistics[name,] = rbind(statistics$name, output)
  
  # add to table
  #name <- deparse(substitute(lexicon))
  #statistics[name] <- c(accuracy, precision, recall, F1)
  #statistics = mutate(statistics$name,c(accuracy, precision, recall, F1) )
  return(statistics)
  
}
# to turn variable name into string 
# deparse(substitute(afinn))

get_statistics(afinn, test$rating_binary, test$afinn_binary)
get_statistics(lsd, test$rating_binary, test$lsd_binary)
get_statistics(vader, test$rating_binary, test$vader_binary)

statistics







statistics <- data.frame(matrix(ncol=4, nrow=3))
x <- c("accuracy", "precision", "recall", "F1")
y <- c("afinn", "lsd", "vader")
rownames(statistics) <- y
colnames(statistics) <- x

```


```{r}
# second try
get_statistics <- function(lex1,lex2,lex3, goldstandard) {
  statistics <- data.frame(matrix(ncol=4, nrow=3))
  x <- c("accuracy", "precision", "recall", "F1")
  y <- c("afinn", "lsd", "vader")
  colnames(statistics) <- x
  rownames(statistics) <- y
    
  lexicons <- list(lex1, lex2, lex3)
  #print(lex1)
  #print(goldstandard)
  #for lex in lexicon, do
  for(lex in lexicons){
    #print(lex)
    confusion_matrix <- table(ACTUAL=goldstandard, PREDICTED=lex)
    TN <- confusion_matrix[1]
    FN <- confusion_matrix[2]
    FP <- confusion_matrix[3]
    TP <- confusion_matrix[4]
  
    # calculate statistics
    precision <- TP/(TP+FP)
    accuracy <- (TP+TN)/(TP+TN+FP+FN)
    recall <- TP/(TP+FN)
    F1 <- (2*precision*recall)/(precision+recall)
  
    # add to table
    #name <- deparse(substitute(lex)) # how to retrieve column name?
    output <- c(accuracy,precision, recall, F1)
    print(name)
    print(output)
    statistics[name,] = rbind(statistics$name, output)
    }
    
  return(statistics)
  
}

get_statistics(test$afinn_binary,test$lsd_binary,test$vader_binary,test$rating_binary)
```


```{r}
lexicons <- list(test$afinn_binary, test$lsd_binary)

for(lex in lexicons){
  print(lex)
}

```






