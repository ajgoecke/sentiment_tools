finance_tok.stopwords <- finance_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
# same as above: if sentiment score is not zero, count as "covered", get percentage of covered tokens
reviews_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(reviews_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok.stopwords != 0)[[1]]*100,2))), "reviews tokens - stopwords")
twitter_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(twitter_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok.stopwords != 0)[[1]]*100,2))), "twitter tokens - stopwords")
parlvote_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok.stopwords != 0)[[1]]*100,2))), "parlvote tokens - stopwords")
amazon_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(amazon_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok.stopwords != 0)[[1]]*100,2))), "amazon tokens - stopwords")
finance_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(finance_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(finance_tok.stopwords != 0)[[1]]*100,2))), "finance tokens - stopwords")
# ===== COVERAGE PER TEXT =====
# Coverage per text: if sentiment score is not zero, count as "covered", get percentage of covered text instances
reviews_coverage <- `rownames<-`(data.frame(t(colSums(reviews_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "reviews text")
twitter_coverage <- `rownames<-`(data.frame(t(colSums(twitter_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "twitter text")
parlvote_coverage <- `rownames<-`(data.frame(t(colSums(parlvote_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "parlvote text")
amazon_coverage <- `rownames<-`(data.frame(t(colSums(amazon_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "amazon text")
finance_coverage <- `rownames<-`(data.frame(t(colSums(finance_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "finance text")
# save data to data frame
coverage <- rbind(reviews_tok_coverage,reviews_tok_coverage.stopwords,reviews_coverage, twitter_tok_coverage,twitter_tok_coverage.stopwords,parlvote_tok_coverage,twitter_coverage,parlvote_tok_coverage.stopwords, parlvote_coverage, amazon_tok_coverage,amazon_tok_coverage.stopwords, amazon_coverage, finance_tok_coverage,finance_tok_coverage.stopwords, finance_coverage)
coverage
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
coverage_per_tok <- rbind(reviews_tok_coverage,twitter_tok_coverage,parlvote_tok_coverage,amazon_tok_coverage, finance_tok_coverage)
coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords, twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
coverage_per_tok <- rbind(reviews_tok_coverage,twitter_tok_coverage,parlvote_tok_coverage,amazon_tok_coverage, finance_tok_coverage)
coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords, twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)
# add corpus names as rownames
coverage_per_tok["corpus"] <- rownames(coverage_per_tok)
coverage_per_tok.stopwords["corpus"] <- rownames(coverage_per_tok.stopwords)
coverage_per_text["corpus"] <- rownames(coverage_per_text)
# prepare data for plotting
cov_tok_df <- melt(coverage_per_tok,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_tok_df.stopwords <- melt(coverage_per_tok.stopwords,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_text_df <- melt(coverage_per_text,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
# create plots
cov_tok_plot <- ggplot(cov_tok_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token")
cov_tok_plot.stopwords <- ggplot(cov_tok_df.stopwords,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token - Stopwords")
cov_text_plot <- ggplot(cov_text_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Text")
cov_tok_plot
cov_tok_plot.stopwords
cov_text_plot
reviews_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(reviews_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok.stopwords != 0)[[1]]*100,2))), "reviews tokens")
twitter_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(twitter_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok.stopwords != 0)[[1]]*100,2))), "twitter tokens")
parlvote_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok.stopwords != 0)[[1]]*100,2))), "parlvote tokens")
amazon_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(amazon_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok.stopwords != 0)[[1]]*100,2))), "amazon tokens")
finance_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(finance_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(finance_tok.stopwords != 0)[[1]]*100,2))), "finance tokens")
reviews_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(reviews_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok.stopwords != 0)[[1]]*100,2))), "reviews tokens (stop)")
twitter_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(twitter_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok.stopwords != 0)[[1]]*100,2))), "twitter tokens (stop)")
parlvote_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok.stopwords != 0)[[1]]*100,2))), "parlvote tokens (stop)")
amazon_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(amazon_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok.stopwords != 0)[[1]]*100,2))), "amazon tokens (stop)")
finance_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(finance_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(finance_tok.stopwords != 0)[[1]]*100,2))), "finance tokens (stop)")
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
coverage_per_tok <- rbind(reviews_tok_coverage,twitter_tok_coverage,parlvote_tok_coverage,amazon_tok_coverage, finance_tok_coverage)
coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords, twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)
# add corpus names as rownames
coverage_per_tok["corpus"] <- rownames(coverage_per_tok)
coverage_per_tok.stopwords["corpus"] <- rownames(coverage_per_tok.stopwords)
coverage_per_text["corpus"] <- rownames(coverage_per_text)
# prepare data for plotting
cov_tok_df <- melt(coverage_per_tok,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_tok_df.stopwords <- melt(coverage_per_tok.stopwords,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_text_df <- melt(coverage_per_text,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
# create plots
cov_tok_plot <- ggplot(cov_tok_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token")
cov_tok_plot.stopwords <- ggplot(cov_tok_df.stopwords,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token - Stopwords")
cov_text_plot <- ggplot(cov_text_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Text")
cov_tok_plot
cov_tok_plot.stopwords
cov_text_plot
# ===== COVERAGE PER TOKEN =====
# Coverage per token: if sentiment score is not zero, count as "covered", get percentage of covered tokens
reviews_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(reviews_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok_sent != 0)[[1]]*100,2))), "reviews tokens")
twitter_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(twitter_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok_sent != 0)[[1]]*100,2))), "twitter tokens")
parlvote_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok_sent != 0)[[1]]*100,2))), "parlvote tokens")
amazon_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(amazon_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok_sent != 0)[[1]]*100,2))), "amazon tokens")
finance_tok_coverage <- `rownames<-`(data.frame(t(round(colSums(finance_tok_sent[c("afinn","lsd","vader")] != 0)/colSums(finance_tok_sent != 0)[[1]]*100,2))), "finance tokens")
# Coverage per token: with stopword removal
# remove stopwords from each data frame
reviews_tok.stopwords <- reviews_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
twitter_tok.stopwords <- twitter_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
parlvote_tok.stopwords <- parlvote_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
amazon_tok.stopwords <- amazon_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
finance_tok.stopwords <- finance_tok_sent %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
# same as above: if sentiment score is not zero, count as "covered", get percentage of covered tokens
reviews_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(reviews_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(reviews_tok.stopwords != 0)[[1]]*100,2))), "reviews tokens (sw)")
twitter_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(twitter_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(twitter_tok.stopwords != 0)[[1]]*100,2))), "twitter tokens (sw)")
parlvote_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(parlvote_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(parlvote_tok.stopwords != 0)[[1]]*100,2))), "parlvote tokens (sw)")
amazon_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(amazon_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(amazon_tok.stopwords != 0)[[1]]*100,2))), "amazon tokens (sw)")
finance_tok_coverage.stopwords <- `rownames<-`(data.frame(t(round(colSums(finance_tok.stopwords[c("afinn","lsd","vader")] != 0)/colSums(finance_tok.stopwords != 0)[[1]]*100,2))), "finance tokens (sw)")
# ===== COVERAGE PER TEXT =====
# Coverage per text: if sentiment score is not zero, count as "covered", get percentage of covered text instances
reviews_coverage <- `rownames<-`(data.frame(t(colSums(reviews_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "reviews text")
twitter_coverage <- `rownames<-`(data.frame(t(colSums(twitter_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "twitter text")
parlvote_coverage <- `rownames<-`(data.frame(t(colSums(parlvote_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "parlvote text")
amazon_coverage <- `rownames<-`(data.frame(t(colSums(amazon_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "amazon text")
finance_coverage <- `rownames<-`(data.frame(t(colSums(finance_sentiment_norm1[c("afinn","lsd","vader")] != 0)/10)), "finance text")
# save data to data frame
coverage <- rbind(reviews_tok_coverage,reviews_tok_coverage.stopwords,reviews_coverage, twitter_tok_coverage,twitter_tok_coverage.stopwords,parlvote_tok_coverage,twitter_coverage,parlvote_tok_coverage.stopwords, parlvote_coverage, amazon_tok_coverage,amazon_tok_coverage.stopwords, amazon_coverage, finance_tok_coverage,finance_tok_coverage.stopwords, finance_coverage)
coverage
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
coverage_per_tok <- rbind(reviews_tok_coverage,twitter_tok_coverage,parlvote_tok_coverage,amazon_tok_coverage, finance_tok_coverage)
coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords, twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)
# add corpus names as rownames
coverage_per_tok["corpus"] <- rownames(coverage_per_tok)
coverage_per_tok.stopwords["corpus"] <- rownames(coverage_per_tok.stopwords)
coverage_per_text["corpus"] <- rownames(coverage_per_text)
# prepare data for plotting
cov_tok_df <- melt(coverage_per_tok,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_tok_df.stopwords <- melt(coverage_per_tok.stopwords,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_text_df <- melt(coverage_per_text,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
# create plots
cov_tok_plot <- ggplot(cov_tok_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token")
cov_tok_plot.stopwords <- ggplot(cov_tok_df.stopwords,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token - Stopwords")
cov_text_plot <- ggplot(cov_text_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Text")
cov_tok_plot
cov_tok_plot.stopwords
cov_text_plot
# ===== PLOT SENTIMENT SCORES =====
# create data frame with sentiment scores as variable of first 100 instances of corpus
reviews_df <- melt(head(reviews_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
twitter_df <- melt(head(twitter_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
parlvote_df <- melt(head(parlvote_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
amazon_df <- melt(head(amazon_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
finance_df <- melt(head(finance_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
# create plots for each corpus
reviews_plot <- ggplot(reviews_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
facet_wrap(~ variable, ncol = 1, scales="free_y")+
xlab("text id")+ ylab("sentiment score")+
ggtitle("Reviews Sentiment")
parlvote_plot <- ggplot(parlvote_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
#facet_wrap(~ variable, ncol = 1, scales="free_y")+
xlab("text id")+ ylab("sentiment score")+
ggtitle("ParlVote Sentiment")
reviews_plot.line <- ggplot(reviews_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-1,1) +
ggtitle("Reviews Sentiment Scores")
reviews_plot.line <- ggplot(reviews_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-1,1) +
ggtitle("Reviews Sentiment Scores")
# show plots
reviews_plot
parlvote_plot
reviews_plot.line
parlvote_plot.line
# ===== PLOT SENTIMENT SCORES =====
# create data frame with sentiment scores as variable of first 100 instances of corpus
reviews_df <- melt(head(reviews_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
twitter_df <- melt(head(twitter_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
parlvote_df <- melt(head(parlvote_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
amazon_df <- melt(head(amazon_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
finance_df <- melt(head(finance_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
# create plots for each corpus
reviews_plot <- ggplot(reviews_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
facet_wrap(~ variable, ncol = 1, scales="free_y")+
xlab("text id")+ ylab("sentiment score")+
ggtitle("Reviews Sentiment")
parlvote_plot <- ggplot(parlvote_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
#facet_wrap(~ variable, ncol = 1, scales="free_y")+
xlab("text id")+ ylab("sentiment score")+
ggtitle("ParlVote Sentiment")
reviews_plot.line <- ggplot(reviews_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-2,2) +
ggtitle("Reviews Sentiment Scores")
parlvote_plot.line <- ggplot(parlvote_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-2,2) +
ggtitle("ParlVote Sentiment Scores")
# show plots
reviews_plot
parlvote_plot
reviews_plot.line
parlvote_plot.line
# ===== PLOT SENTIMENT SCORES =====
# create data frame with sentiment scores as variable of first 100 instances of corpus
reviews_df <- melt(head(reviews_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
twitter_df <- melt(head(twitter_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
parlvote_df <- melt(head(parlvote_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
amazon_df <- melt(head(amazon_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
finance_df <- melt(head(finance_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
# create plots for each corpus
reviews_plot <- ggplot(reviews_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
xlab("text id")+ ylab("sentiment score")+
ggtitle("Reviews Sentiment")
parlvote_plot <- ggplot(parlvote_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge")+
#facet_wrap(~ variable, ncol = 1, scales="free_y")+
xlab("text id")+ ylab("sentiment score")+
ggtitle("ParlVote Sentiment")
reviews_plot.line <- ggplot(reviews_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-2,2) +
ggtitle("Reviews Sentiment Scores")
parlvote_plot.line <- ggplot(parlvote_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-2,2) +
ggtitle("ParlVote Sentiment Scores")
# show plots
reviews_plot
parlvote_plot
reviews_plot.line
parlvote_plot.line
# convert values to discrete format
reviews_tok_discrete <- get_discrete(reviews_tok_sent, c("afinn","vader","lsd"))
twitter_tok_discrete <- get_discrete(twitter_tok_sent, c("afinn","vader","lsd"))
parlvote_tok_discrete <- get_discrete(parlvote_tok_sent, c("afinn","vader","lsd"))
amazon_tok_discrete <- get_discrete(amazon_tok_sent, c("afinn","vader","lsd"))
finance_tok_discrete <- get_discrete(finance_tok_sent, c("afinn","vader","lsd"))
# ===== PLOT SENTIMENT SCORES =====
# create data frame with sentiment scores as variable of first 100 instances of corpus
reviews_df <- melt(head(reviews_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
parlvote_df <- melt(head(parlvote_sentiment_norm1,100)[,c('id','afinn','lsd','vader')],id.vars = 1)
# create plots for each corpus
reviews_plot <- ggplot(reviews_df,aes(x = id,y = value)) +
geom_bar(aes(fill = variable),stat = "identity",position = "dodge") +
xlab("text id")+ ylab("sentiment score")+
ggtitle("Reviews Sentiment")
parlvote_plot.line <- ggplot(parlvote_df,aes(x = id, y = value, group=variable)) +
geom_line(aes(colour=variable), size=0.4)+
ylim(-2,2) +
ggtitle("ParlVote Sentiment Scores")
# show plots
reviews_plot
parlvote_plot.line
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
# ===== REVIEWS DATA =====
# get word counts per discrete variable ("positive", "negative", "neutral", i.e. 1,-1,0)
reviews_afinn.word_counts <- reviews_tok_discrete %>%
count(token, afinn, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
reviews_lsd.word_counts <- reviews_tok_discrete %>%
count(token, lsd, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
reviews_vader.word_counts <- reviews_tok_discrete %>%
count(token, vader, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
# ===== PARLVOTE DATA =====
parlvote_afinn.word_counts <- parlvote_tok_discrete %>%
count(token, afinn, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
parlvote_lsd.word_counts <- parlvote_tok_discrete %>%
count(token, lsd, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
parlvote_vader.word_counts <- parlvote_tok_discrete %>%
count(token, vader, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
group_by(lsd) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = lsd)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~lsd, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
group_by(vader) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = vader)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~vader, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")
topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
# convert values to discrete format
reviews_tok_discrete <- get_discrete(reviews_tok_sent, c("afinn","vader","lsd"))
# get word counts per discrete variable ("positive", "negative", "neutral", i.e. 1,-1,0)
reviews_afinn.word_counts <- reviews_tok_discrete %>%
count(token, afinn, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
reviews_lsd.word_counts <- reviews_tok_discrete %>%
count(token, lsd, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
reviews_vader.word_counts <- reviews_tok_discrete %>%
count(token, vader, sort = TRUE) %>%
anti_join(stop_words, by= c("token" = "word") ) %>%
ungroup()
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
group_by(lsd) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = lsd)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~lsd, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
group_by(vader) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = vader)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~vader, scales = "free_y") +
labs(y = "contribution to sentiment", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")
topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
group_by(lsd) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = lsd)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~lsd, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
group_by(vader) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = vader)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~vader, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")
topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
group_by(lsd) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = lsd)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~lsd, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
group_by(vader) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = vader)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
#facet_wrap(~vader, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")
topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
# plot top n words for each sentiment and for each lexicon, n = 20
topn_reviews_afinn.plot <- reviews_afinn.word_counts %>%
group_by(afinn) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = afinn)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~afinn, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (Afinn)")
topn_reviews_lsd.plot <- reviews_lsd.word_counts %>%
group_by(lsd) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = lsd)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~lsd, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (LSD)")
topn_reviews_vader.plot <- reviews_vader.word_counts %>%
group_by(vader) %>%
slice(1:20) %>%
ggplot(aes(reorder(token, n), n, fill = vader)) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~vader, scales = "free_y") +
labs(y = "word frequency", x = NULL) +
coord_flip()+
ggtitle("Reviews: Top 20 Words per Sentiment (VADER)")
topn_reviews_afinn.plot
topn_reviews_lsd.plot
topn_reviews_vader.plot
# ===== PREPARE DATA FOR PLOTTING =====
# create separate data frames we want to plot
coverage_per_text <- rbind(reviews_coverage,twitter_coverage,parlvote_coverage,amazon_coverage,finance_coverage)
coverage_per_tok <- rbind(reviews_tok_coverage,twitter_tok_coverage,parlvote_tok_coverage,amazon_tok_coverage, finance_tok_coverage)
coverage_per_tok.stopwords <- rbind(reviews_tok_coverage.stopwords, twitter_tok_coverage.stopwords,parlvote_tok_coverage.stopwords,amazon_tok_coverage.stopwords,finance_tok_coverage.stopwords)
# add corpus names as row names
coverage_per_tok["corpus"] <- rownames(coverage_per_tok)
coverage_per_tok.stopwords["corpus"] <- rownames(coverage_per_tok.stopwords)
coverage_per_text["corpus"] <- rownames(coverage_per_text)
# prepare data for plotting
cov_tok_df <- melt(coverage_per_tok,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_tok_df.stopwords <- melt(coverage_per_tok.stopwords,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
cov_text_df <- melt(coverage_per_text,id.vars = "corpus", value.name="coverage", variable.name="Lexicon")
# ===== COVERAGE PLOTS =====
# create plots
cov_tok_plot <- ggplot(cov_tok_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token")
cov_tok_plot.stopwords <- ggplot(cov_tok_df.stopwords,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Token - Stopwords")
cov_text_plot <- ggplot(cov_text_df,aes(x=corpus, y = coverage)) +
geom_bar(aes(fill = Lexicon),stat = "identity",position = "dodge", width= 0.7) +
xlab("Corpus")+ ylab("Coverage in %")+
ggtitle("Coverage per Text")
# show plots
cov_tok_plot
cov_tok_plot.stopwords
cov_text_plot
detach("package:caret", unload = TRUE)
detach("package:corpus", unload = TRUE)
detach("package:dplyr", unload = TRUE)
detach("package:ggplot2", unload = TRUE)
detach("package:harrypotter", unload = TRUE)
detach("package:quanteda", unload = TRUE)
detach("package:quanteda.sentiment", unload = TRUE)
